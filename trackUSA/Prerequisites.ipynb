{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   * * *   Lancement de Prerequisites.py   * * *   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"   * * *   Lancement de Prerequisites.py   * * *   \")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dans le gdic on doit absolument ajouter les bornes aux deux dataframes construits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output, display\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "french_stopwords = list(fr_stop)\n",
    "from scipy import stats\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import igraph as ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadJsonFile(filename): \n",
    "    with open(filename, 'r') as f:\n",
    "        DicConfig = json.load(f)\n",
    "    return DicConfig\n",
    "\n",
    "\n",
    "def GlobalDicDeplier(OneDic):\n",
    "    for k,v in OneDic.items():\n",
    "        exec('globals()[k] = v')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier config\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement du fichier config\")\n",
    "\n",
    "DicConfig = LoadJsonFile(os.path.join(os.getcwd(),\"config.json\"))\n",
    "GlobalDicDeplier(DicConfig)\n",
    "sys.path.append(Root)\n",
    "from fun import *\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donnés\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement des donnés\")\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"FinalFam.pkl\")\n",
    "FinalFam = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"FinalInf.pkl\")\n",
    "FinalInf = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"gdic.pkl\")\n",
    "gdic = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"tfidf_DocsRepresentation.pkl\")\n",
    "DocsRep = LoadPickleOrInit(path)\n",
    "DocsRep.rename(columns = {\"f\":\"ndocs\",\"tweetid\":\"AUTHORTWEETID\"},inplace=True)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"tmdic.pkl\")\n",
    "tmdic = LoadPickleOrInit(path)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des fonctions\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement des fonctions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeStatsWord(DocsRep,gtemp):\n",
    "\n",
    "    statstemp= DocsRep.merge(gtemp,on=\"AUTHORTWEETID\")\n",
    "    statstemp[\"power\"] = statstemp.idf * statstemp.f\n",
    "    statswords = statstemp.groupby(\"word\")[\"power\"].sum().reset_index()\n",
    "    statswords.sort_values(by=\"power\",inplace=True)\n",
    "    statswords.reset_index(drop=True,inplace=True)\n",
    "    statswords[\"powernormalized\"] = statswords.power / statswords.power.sum()\n",
    "\n",
    "    return statswords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinksFromDicToDF(randomlinksdf):\n",
    "\n",
    "    L = []\n",
    "    for k,v in randomlinksdf.items():\n",
    "        a = k[0]\n",
    "        b = k[1]\n",
    "        f = v\n",
    "        tempdic = {\"a\":a,\"b\":b,\"f\":f}\n",
    "        L.append(tempdic)\n",
    "\n",
    "    solution = pd.DataFrame(L)\n",
    "\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildAndSavePRQDF(gdic,DocsRep,Root,FolderProject,FolderPRQ):\n",
    "    \n",
    "    compteur = 0\n",
    "    linksrows = 0\n",
    "    statsrows = 0\n",
    "    for k,v in gdic.items():\n",
    "        if \"ondisk\" not in gdic[k]:\n",
    "            linkscsv = LinksFromDicToDF(v[\"links\"])\n",
    "            linkscsv[\"tmstr\"] = str(v[\"tmstr\"])\n",
    "            linkscsv[\"start\"] = v[\"GraphStart\"]\n",
    "            linkscsv[\"end\"] = v[\"tm\"]\n",
    "            path = os.path.join(Root,FolderProject,FolderPRQ,\"graph\"+str(k)+\".csv\")\n",
    "            linkscsv.to_csv(path,index=False)\n",
    "            linksrows = linksrows + len(linkscsv)\n",
    "\n",
    "            gtemp = v[\"informations\"]\n",
    "            statswords = ComputeStatsWord(DocsRep,gtemp)\n",
    "            statswords[\"tmstr\"] = str(v[\"tmstr\"])\n",
    "            statswords[\"start\"] = v[\"TFIDFStart\"]\n",
    "            statswords[\"end\"] = v[\"tm\"]\n",
    "            path = os.path.join(Root,FolderProject,FolderPRQ,\"tfidf\"+str(k)+\".csv\")\n",
    "            statswords.to_csv(path,index=False)\n",
    "            statsrows = statsrows + len(statswords)\n",
    "\n",
    "            gdic[k][\"ondisk\"] = True\n",
    "            compteur = compteur + 1\n",
    "        \n",
    "    return compteur,linksrows,statsrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractNextBorne(SelectedID,x,verbose=False):\n",
    "\n",
    "    try:\n",
    "        InfValue = x[SelectedID][1]\n",
    "        if verbose:\n",
    "            print(InfValue)\n",
    "        \n",
    "        ChampDesPossibles = np.array([item[0] for item in x[SelectedID + 1 :len(x)]])\n",
    "        if verbose:\n",
    "            print(ChampDesPossibles)\n",
    "        \n",
    "        Distance = np.abs(InfValue - ChampDesPossibles)\n",
    "        if verbose:\n",
    "            print(Distance)\n",
    "        \n",
    "        idres = np.argmin(Distance) + SelectedID + 1 \n",
    "        if verbose:\n",
    "            print(idres)\n",
    "        \n",
    "        res = x[idres]\n",
    "        if verbose:\n",
    "            print(res)\n",
    "        \n",
    "    except:\n",
    "        idres = None\n",
    "        res = None\n",
    "    \n",
    "    return idres,res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractAllBornes(x):\n",
    "\n",
    "    \"\"\"\n",
    "    Input : liste de tuples représentant les bornes\n",
    "    Output : liste de tuples représentant les bornes les plus revelantes\n",
    "    \"\"\"\n",
    "    \n",
    "    x.sort(key = lambda a : a[1], reverse = False)\n",
    "    SelectedID = 0\n",
    "    borne = x[SelectedID]\n",
    "    L = [borne]\n",
    "\n",
    "    while True:\n",
    "        idres,res = ExtractNextBorne(SelectedID,x)\n",
    "        if idres is not None:\n",
    "            SelectedID = idres\n",
    "            L.append(res)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildGraphFrom2DF(FinalInf,graphdf):\n",
    "\n",
    "    \"\"\"\n",
    "    Input : un dataframe des nodes\n",
    "    Input : un dataframe des links\n",
    "    Output : un objet graph igraph\n",
    "    \"\"\"\n",
    "    \n",
    "    g = ig.Graph(directed=False)\n",
    "    nodesids = FinalInf.GRAPHID.values\n",
    "    g.add_vertices(nodesids)\n",
    "\n",
    "    g.vs[\"AUTHORDESCRIPTION\"] = FinalInf.AUTHORDESCRIPTION.tolist()\n",
    "    g.vs[\"AUTHORFNAME\"] = FinalInf.AUTHORFNAME.tolist()\n",
    "    g.vs[\"AUTHORID\"] = FinalInf.AUTHORID.tolist()\n",
    "    g.vs[\"AUTHORNAME\"] = FinalInf.AUTHORNAME.tolist()\n",
    "    g.vs[\"AUTHORFOLLOWERS\"] = FinalInf.AUTHORFOLLOWERS.tolist()\n",
    "\n",
    "    edgeslist = graphdf[[\"GRAPHIDA\",\"GRAPHIDB\"]].values.tolist()\n",
    "    g.add_edges(edgeslist)\n",
    "    g.es[\"weight\"] = graphdf.f.tolist()\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def DeleteSomeNodes(g,AttributeName,ThresholdFollowersInf,ThresholdFollowersSup):\n",
    "\n",
    "    \"\"\"\n",
    "    Input : un objet igraph graph\n",
    "    Input : borne inferieur et superieur\n",
    "    Input : Attribute Name\n",
    "    Output : un objet igraph graph\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    to_delete_ids = [node for node in g.vs if node[AttributeName]<ThresholdFollowersInf]\n",
    "    g.delete_vertices(to_delete_ids)\n",
    "\n",
    "    to_delete_ids = [node for node in g.vs if node[AttributeName]>ThresholdFollowersSup]\n",
    "    g.delete_vertices(to_delete_ids)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumUpDics(listofdics):\n",
    "    \n",
    "    mydics = [Counter(item) for item in listofdics]\n",
    "    c = Counter()\n",
    "    \n",
    "    for d in mydics:\n",
    "        c.update(d)\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildGlobalLinksDF(gdic):\n",
    "\n",
    "    BornesList = [(v[\"GraphStart\"],v[\"tm\"]) for k,v in gdic.items()]\n",
    "    MyBornes = ExtractAllBornes(BornesList)\n",
    "    TimeMarkList = [item[1] for item in MyBornes]\n",
    "    ListOfSelectedDataFrames = [gdic[item][\"links\"] for item in gdic.keys() if item in TimeMarkList]\n",
    "    GlobalLinksdf = dict(SumUpDics(ListOfSelectedDataFrames))\n",
    "    GlobalBorne = (MyBornes[0][0],MyBornes[-1][1])\n",
    "    GlobalLinksdf = LinksFromDicToDF(GlobalLinksdf)\n",
    "    GlobalLinksdf = GlobalLinksdf.sort_values(by=\"f\",ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return GlobalBorne,GlobalLinksdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde des fichiers PRQ, input de l'app Shiny\n",
      "Nombre de dataframes rajoutés :  0\n",
      "Nombre de lignes rajoutées, links :  0\n",
      "Nombre de lignes rajoutées, statswords :  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sauvegarde des fichiers PRQ, input de l'app Shiny\")\n",
    "\n",
    "compteur,linksrows,statsrows = BuildAndSavePRQDF(gdic,DocsRep,Root,FolderProject,FolderPRQ)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,FolderPRQ,\"FinalFam.csv\")\n",
    "FinalFam.to_csv(path,index=False)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,FolderPRQ,\"FinalInf.csv\")\n",
    "FinalInf.to_csv(path,index=False)\n",
    "\n",
    "PickleDump(os.path.join(Root,FolderProject,\"gdic.pkl\"),gdic)\n",
    "print(\"Nombre de dataframes rajoutés : \",compteur)\n",
    "print(\"Nombre de lignes rajoutées, links : \",linksrows)\n",
    "print(\"Nombre de lignes rajoutées, statswords : \",statsrows)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalBorne,GlobalLinksdf = BuildGlobalLinksDF(gdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphdf = GlobalLinksdf.copy()\n",
    "graphdf.sort_values(by=\"f\",ascending=False,inplace=True)\n",
    "nodesids = np.unique(np.hstack((graphdf.a.values,graphdf.b.values)))\n",
    "\n",
    "FinalInf = PickleLoad(\"FinalInf.pkl\")\n",
    "FinalInf = FinalInf[FinalInf.AUTHORID.isin(nodesids)]\n",
    "FinalInf = FinalInf.reset_index(drop=True).reset_index()\n",
    "FinalInf.rename(columns = {\"index\":\"GRAPHID\"},inplace=True)\n",
    "\n",
    "graphdf = graphdf.merge(FinalInf[[\"GRAPHID\",\"AUTHORID\"]],how=\"left\",left_on=\"a\",right_on=\"AUTHORID\").rename(columns={\"GRAPHID\":\"GRAPHIDA\"})\n",
    "graphdf = graphdf.merge(FinalInf[[\"GRAPHID\",\"AUTHORID\"]],how=\"left\",left_on=\"b\",right_on=\"AUTHORID\").rename(columns={\"GRAPHID\":\"GRAPHIDB\"})\n",
    "graphdf.drop(columns=[\"AUTHORID_x\",\"AUTHORID_y\"],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Parameters\n",
    "\n",
    "ThresholdFollowersInf = 40000\n",
    "ThresholdFollowersSup = 80000\n",
    "maxComponents = 6\n",
    "maxWeight = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BuildGraphFrom2DF(FinalInf,graphdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BuildGraphFrom2DF(FinalInf,graphdf)\n",
    "todelete = [edge for edge in g.es if edge[\"weight\"]<maxWeight]\n",
    "g.delete_edges(todelete)\n",
    "g = DeleteSomeNodes(g,\"AUTHORFOLLOWERS\",ThresholdFollowersInf,ThresholdFollowersSup)\n",
    "todelete = [node for node in g.vs if node.degree()==0]\n",
    "g.delete_vertices(todelete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer le dataframe, seulement le top5 des components\n",
    "\n",
    "ComponentsSize = np.array([len(item) for item in g.components()])\n",
    "ComponentsSize = np.sort(ComponentsSize)[::-1]\n",
    "size,freq = np.unique(ComponentsSize,return_counts=True)\n",
    "size = size[::-1]\n",
    "freq = freq[::-1]\n",
    "nodesids = [item for item in g.components() if len(item) in list(size[freq.cumsum()<maxComponents])]\n",
    "nodesids = [item for sublist in nodesids for item in sublist]\n",
    "todelete = [node for node in g.vs if node.index not in nodesids]\n",
    "g.delete_vertices(todelete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "\n",
    "clustering = g.community_multilevel(weights=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction des nodesdf et linksdf\n",
    "\n",
    "nodesdf = pd.Series(clustering.membership).to_frame(name=\"CLUSTERID\").reset_index().rename(columns = {\"index\":\"GRAPHINDEX\"})\n",
    "nodesdf[\"GRAPHID\"] = pd.Series(g.vs[\"name\"])\n",
    "nodesdf[\"AUTHORID\"] = pd.Series(g.vs[\"AUTHORID\"])\n",
    "nodesdf[\"AUTHORFNAME\"] = pd.Series(g.vs[\"AUTHORFNAME\"])\n",
    "nodesdf[\"AUTHORNAME\"] = pd.Series(g.vs[\"AUTHORNAME\"])\n",
    "nodesdf[\"AUTHORFOLLOWERS\"] = pd.Series(g.vs[\"AUTHORFOLLOWERS\"])\n",
    "nodesdf[\"PERIODINF\"] = GlobalBorne[0]\n",
    "nodesdf[\"PERIODSUP\"] = GlobalBorne[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(item.source,item.target,item.attributes()[\"weight\"]) for item in g.es]\n",
    "linksdf = pd.DataFrame(data, columns=['a', 'b', 'weight'])\n",
    "\n",
    "linksdf = linksdf.\\\n",
    "merge(nodesdf[[\"GRAPHINDEX\",\"AUTHORID\"]],left_on=\"a\",right_on=\"GRAPHINDEX\",how=\"left\").\\\n",
    "drop(columns=[\"GRAPHINDEX\"]).\\\n",
    "rename(columns = {\"AUTHORID\":\"AUTHORIDA\"})\n",
    "\n",
    "linksdf = linksdf.\\\n",
    "merge(nodesdf[[\"GRAPHINDEX\",\"AUTHORID\"]],left_on=\"b\",right_on=\"GRAPHINDEX\",how=\"left\").\\\n",
    "drop(columns=[\"GRAPHINDEX\"]).\\\n",
    "rename(columns = {\"AUTHORID\":\"AUTHORIDB\"})\n",
    "\n",
    "linksdf[\"PERIODINF\"] = GlobalBorne[0]\n",
    "linksdf[\"PERIODSUP\"] = GlobalBorne[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des dataframes sur le disk\n",
    "nodesdf.to_csv(os.path.join(Root,FolderProject,FolderPRQ,\"nodesdf.csv\"),index=False)\n",
    "linksdf.to_csv(os.path.join(Root,FolderProject,FolderPRQ,\"linksdf.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecriture des logs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ecriture des logs\")\n",
    "\n",
    "files = os.listdir(os.path.join(Root,FolderProject,FolderPRQ))\n",
    "graphfilesnumber = len([item.startswith(\"graph\") for item in files])\n",
    "tfidffilesnumber = len([item.startswith(\"tfidf\") for item in files])\n",
    "nvertex = len(g.vs)\n",
    "nedges = len(g.es)\n",
    "nclusters = len(np.unique(np.array(clustering.membership)))\n",
    "ncompo = len(g.components())\n",
    "\n",
    "log = {\n",
    "    \"Date\":GetCurrentTime(),\n",
    "    \"Nombre de graph dataframes crées\" : compteur,\n",
    "    \"Nombre de lignes total rajoutées, linksdf\":linksrows,\n",
    "    \"Nombre de lignes total rajoutées, statswords\":statsrows,\n",
    "    \"Nombre de tweets, FinalFam\":len(FinalFam),\n",
    "    \"Nombre d'auteurs, FinalInf\":len(FinalInf),\n",
    "    \"Nombre de dataframes de type graph so far\":graphfilesnumber,\n",
    "    \"Nombre de dataframes de type tfidf so far\":tfidffilesnumber,\n",
    "    \"Nombre de vertex dans le graph global\":nvertex,\n",
    "    \"Nombre de links dans le graph global\":nedges,\n",
    "    \"Nombre de communautés dans le graph global\":nclusters,\n",
    "    \"Nombre de composantes principales dans le graph global\":ncompo\n",
    "}\n",
    "\n",
    "filename = os.path.join(Root,FolderProject,\"PRQ.log\")\n",
    "AppendStringToFile(filename,log)\n",
    "\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
