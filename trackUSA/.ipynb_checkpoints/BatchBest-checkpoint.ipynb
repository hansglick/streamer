{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   * * *   Lancement du BatchBest.py   * * *   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"   * * *   Lancement du BatchBest.py   * * *   \")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output, display\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadJsonFile(filename): \n",
    "    with open(filename, 'r') as f:\n",
    "        DicConfig = json.load(f)\n",
    "    return DicConfig\n",
    "\n",
    "def GlobalDicDeplier(OneDic):\n",
    "    for k,v in OneDic.items():\n",
    "        exec('globals()[k] = v')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier config\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement du fichier config\")\n",
    "\n",
    "DicConfig = LoadJsonFile(os.path.join(os.getcwd(),\"config.json\"))\n",
    "GlobalDicDeplier(DicConfig)\n",
    "sys.path.append(Root)\n",
    "from fun import *\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement des données\")\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"RefFam.pkl\")\n",
    "RefFam = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"RefRT.pkl\")\n",
    "RefRT = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"RefInf.pkl\")\n",
    "RefInf = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"tmdic.pkl\")\n",
    "tmdic = LoadPickleOrInit(path,typeobj=\"dic\")\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"FinalFam.pkl\")\n",
    "FinalFam = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"FinalRT.pkl\")\n",
    "FinalRT = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"FinalInf.pkl\")\n",
    "FinalInf = LoadPickleOrInit(path)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul des statistiques pour les logs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Calcul des statistiques pour les logs\")\n",
    "\n",
    "FamOriginSize = len(FinalFam)\n",
    "RTOriginSize = len(FinalRT)\n",
    "InfOriginSize = len(FinalInf)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informations quant aux prochains time mark\n",
      "Date du du potentiel next Time Mark :  2020-02-10 20:44:03\n",
      "Date 1er retweet :  2020-02-10 20:24:18\n",
      "Date Dernier retweet :  2020-02-10 21:52:46\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Informations quant aux prochains time mark\")\n",
    "nexttmsaved = getnexttm(RefRT,tmdic,StepSize,WindowSize,verbose = True)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupérer le next batch de rt et la maj des retweets dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **rtdf** : dataframe de reference des retweets téléchargés. Le fichier est alimenté régulièrement.\n",
    " * **pbest** : proportion de volume de retweets à garder\n",
    " * **tmdic** : dictionnaire dont les clés sont des timemarks, les valeurs peuvent être les bornes inférieurs et supérieurs\n",
    " * **rtdf_period** : dataframe de reference des retweets téléchargés uniquement sur une période\n",
    " * **stepsize** : taille du step de la fenetre glissante en secondes\n",
    " * **windowsize** : taille de la fenêtre glissante en secondes\n",
    " * **tm** : une timemark en secondes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'Analytics ...\n",
      "Nombre de batch rajoutés :  14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Début de l'Analytics ...\")\n",
    "\n",
    "tocontinue = True\n",
    "compteur = 0\n",
    "while(tocontinue):\n",
    "    bestrtdf,bestfamdf,bestinfdf,informations,nexttm = getbestrtbashdic(RefRT,\n",
    "                                                                        tmdic,\n",
    "                                                                        StepSize,\n",
    "                                                                        WindowSize,\n",
    "                                                                        TopTweetsProportion,\n",
    "                                                                        RefFam,\n",
    "                                                                        RefInf)\n",
    "    \n",
    "    if nexttm is not None:\n",
    "        compteur = compteur + 1\n",
    "        tmdic[nexttm] = informations\n",
    "        FinalInf = pd.concat((FinalInf,bestinfdf),axis = 0, sort = True)\n",
    "        FinalFam = pd.concat((FinalFam,bestfamdf),axis = 0, sort = True)\n",
    "        FinalRT = pd.concat((FinalRT,bestrtdf),axis = 0, sort = True)\n",
    "\n",
    "        FinalRT = FinalRT.drop_duplicates()\n",
    "        FinalFam = FinalFam.drop_duplicates(subset=[\"AUTHORTWEETID\"])\n",
    "        FinalInf = FinalInf.drop_duplicates(subset=[\"AUTHORID\"])\n",
    "\n",
    "        FinalInf.reset_index(drop=True,inplace=True)\n",
    "        FinalFam.reset_index(drop=True,inplace=True)\n",
    "        FinalRT.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "    else:\n",
    "        tocontinue = False\n",
    "        \n",
    "print(\"Nombre de batch rajoutés : \", compteur)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde du dictionnaire tmdic et des dataframes Finalx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if compteur > 0 :\n",
    "    print(\"Sauvegarde du dictionnaire tmdic et des dataframes Finalx\")\n",
    "    \n",
    "    PickleDump(os.path.join(Root,FolderProject,\"tmdic.pkl\"),tmdic)\n",
    "    PickleDump(os.path.join(Root,FolderProject,\"FinalInf.pkl\"),FinalInf)\n",
    "    PickleDump(os.path.join(Root,FolderProject,\"FinalFam.pkl\"),FinalFam)\n",
    "    PickleDump(os.path.join(Root,FolderProject,\"FinalRT.pkl\"),FinalRT)\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérification cohérence\n",
      "Fichier cohérent True\n",
      "taille :  5802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Vérification cohérence\")\n",
    "\n",
    "tweetsrt = pd.Series(FinalRT.AUTHORTWEETID.unique()).to_frame(name=\"AUTHORTWEETID\")\n",
    "solution = tweetsrt.merge(FinalFam,on=\"AUTHORTWEETID\")\n",
    "\n",
    "print(\"Fichier cohérent\",tweetsrt.shape[0] == solution.shape[0])\n",
    "print(\"taille : \", len(tweetsrt))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeur du seuil en dessous duquel on peut supprimer une partie de la table RefRT:\n",
      "2020-02-10 21:34:03\n",
      "\n",
      "Modification de RefRT, table de référence des retweets\n",
      "\n",
      "Modification du fichier RefFam, table de référence des tweets repris\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Valeur du seuil en dessous duquel on peut supprimer une partie de la table RefRT:\")\n",
    "\n",
    "threshold = retrievethreshold(tmdic,WindowSize,RefRT)\n",
    "\n",
    "print(str(pd.to_datetime(threshold,unit=\"s\")))\n",
    "print(\"\")\n",
    "\n",
    "if threshold is not None : \n",
    "    RemoveRT,KeepRT = extractkeepremove(RefRT,threshold)\n",
    "    PickleDump(os.path.join(Root,FolderProject,\"RefRT.pkl\"),KeepRT)\n",
    "    print(\"Modification de RefRT, table de référence des retweets\")\n",
    "    print(\"\")\n",
    "else:\n",
    "    KeepRT = pd.DataFrame()\n",
    "    \n",
    "if threshold is not None:\n",
    "    fil = RefRT.AUTHORTWEETID>=threshold\n",
    "    tempdf = pd.Series(RefRT.AUTHORTWEETID[fil].unique()).to_frame(name=\"AUTHORTWEETID\")\n",
    "    toadd = tempdf.merge(RefFam,on=\"AUTHORTWEETID\")\n",
    "    NewRefFam = pd.concat((toadd,FinalFam),axis=0,sort=True).drop_duplicates(subset=\"AUTHORTWEETID\")\n",
    "    \n",
    "if threshold is not None:\n",
    "    PickleDump(os.path.join(Root,FolderProject,\"RefFam.pkl\"),NewRefFam)\n",
    "    print(\"Modification du fichier RefFam, table de référence des tweets repris\")\n",
    "    print(\"\")\n",
    "else:\n",
    "    NewRefFam = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecriture des logs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ecriture des logs\")\n",
    "\n",
    "mylog = {\n",
    "\"Date\":GetCurrentTime(),\n",
    "\"Nombre de batchs rajoutés\" : compteur,\n",
    "\"Taille du dictionnaire tmdic\" : len(tmdic),\n",
    "\"Le premier time mark\" : nexttmsaved,\n",
    "\"Nombre de lignes rajoutées dans la table RefFam\":len(RefFam) - FamOriginSize,\n",
    "\"Nombre de lignes rajoutées dans la table RefRT\":len(RefRT) - RTOriginSize,\n",
    "\"Nombre lignes rajoutées dans la table RefInf\":len(RefInf) - InfOriginSize,\n",
    "\"Suppression des lignes dans la table RefRT\" : len(RefRT) - len(KeepRT),\n",
    "\"Suppression des lignes dans la table RefFam\":len(RefFam)  - len(NewRefFam)\n",
    "}\n",
    "\n",
    "filename = os.path.join(Root,FolderProject,\"Best.log\")\n",
    "AppendStringToFile(filename,mylog)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
