{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output, display\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "french_stopwords = list(fr_stop)\n",
    "from scipy import stats\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"déjà\",\n",
    "\"avez\",\n",
    "\"faut\",\n",
    "\"êtes\",\n",
    "\"faire\"}\n",
    "fr_stop = fr_stop.union(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadJsonFile(filename): \n",
    "    with open(filename, 'r') as f:\n",
    "        DicConfig = json.load(f)\n",
    "    return DicConfig\n",
    "\n",
    "\n",
    "def GlobalDicDeplier(OneDic):\n",
    "    for k,v in OneDic.items():\n",
    "        exec('globals()[k] = v')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier config\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement du fichier config\")\n",
    "DicConfig = LoadJsonFile(os.path.join(os.getcwd(),\"config.json\"))\n",
    "GlobalDicDeplier(DicConfig)\n",
    "sys.path.append(Root)\n",
    "from fun import *\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donnés\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Chargement des donnés\")\n",
    "# LOAD DATA\n",
    "path = os.path.join(Root,FolderProject,\"FinalFam.pkl\")\n",
    "FinalFam = LoadPickleOrInit(path)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction de tweets analyse dataframe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Construction de tweets analyse dataframe\")\n",
    "TweetsToAnalyse = FinalFam.copy()[[\"AUTHORTWEETID\",\"AUTHORTWEETCONTENT\",\"AUTHORTWEETUNIXEPOCH\"]].\\\n",
    "rename(columns = {\"AUTHORTWEETID\":\"TWEETID\",\"AUTHORTWEETCONTENT\":\"TWEETCONTENT\"})\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExploreTweetTreatment(mytweetid,reffam,tfidf):\n",
    "    content = reffam.AUTHORTWEETCONTENT[reffam.AUTHORTWEETID==mytweetid].iloc[0]\n",
    "    tokens = Tokenizer(content)\n",
    "    details = tfidf[tfidf.tweetid==mytweetid][[\"word\",\"idf\",\"Rank\",\"f\",\"w\"]].\\\n",
    "    to_dict(orient=\"records\")\n",
    "    return content,tokens,details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \n",
    "    # Initialisation de l'objet\n",
    "    def __init__(self,TooFrequentThreshold,TooInfrequentThreshold):\n",
    "        \n",
    "        self.WORDS2TWEETS = pd.DataFrame()\n",
    "        self.DOCSTOUCHED = pd.DataFrame()\n",
    "        self.COMPTEUR = {}\n",
    "        \n",
    "        self.BATCH_WORDS2TWEETS = pd.DataFrame()\n",
    "        self.BATCH_DOCSTOUCHED = pd.DataFrame()\n",
    "        self.BATCH_COMPTEUR = {}\n",
    "        self.BATCH_V = []\n",
    "        \n",
    "        self.DocsRepresentation = pd.DataFrame()\n",
    "        self.TooFrequentThreshold = TooFrequentThreshold\n",
    "        self.TooInfrequentThreshold = TooInfrequentThreshold\n",
    "        self.LastDate = 0\n",
    "     \n",
    "    \n",
    "    def LoadData(self):\n",
    "        \n",
    "        self.WORDS2TWEETS = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_words2tweets.pkl\"))\n",
    "        self.DOCSTOUCHED = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_docstouched.pkl\"))\n",
    "        self.COMPTEUR = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_compteur.pkl\"),typeobj=\"dic\")\n",
    "        self.DocsRepresentation = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_DocsRepresentation.pkl\"))\n",
    "        self.LastDate = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_lastdate.pkl\"),typeobj=\"0\")\n",
    "        \n",
    "        if len(self.DOCSTOUCHED)==0:\n",
    "            print(\"No data!\")\n",
    "\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def SaveOnDisk(self):\n",
    "        \n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_compteur.pkl\"),self.COMPTEUR)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_docstouched.pkl\"),self.DOCSTOUCHED)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_words2tweets.pkl\"),self.WORDS2TWEETS)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_DocsRepresentation.pkl\"),self.DocsRepresentation)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_lastdate.pkl\"),self.LastDate)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # EVALUATION de la variable globale |D|\n",
    "    # i.e. le nombre de documents, i.e. les tweets\n",
    "    def EvaluateD(self):\n",
    "        if len(self.WORDS2TWEETS)>0:\n",
    "            res = len(np.unique(self.WORDS2TWEETS.tweetid))\n",
    "        else:\n",
    "            res = 0\n",
    "        return res\n",
    "    \n",
    "    # CREATION de la liste de batch\n",
    "    def ProcessCorpus(self,TweetsDataFrame):\n",
    "        \n",
    "        tweetsdf = TweetsDataFrame.copy()\n",
    "        \n",
    "        if len(self.WORDS2TWEETS)>0:\n",
    "            a = self.WORDS2TWEETS.tweetid.unique()\n",
    "            b = tweetsdf.TWEETID\n",
    "            fil = ~b.isin(a)\n",
    "            tweetsdf = tweetsdf[fil]\n",
    "            \n",
    "        tweetsdf = tweetsdf.copy()[tweetsdf.AUTHORTWEETUNIXEPOCH>self.LastDate]\n",
    "        \n",
    "        \n",
    "        print(\"Nombre de retweets à traier : \",len(tweetsdf))\n",
    "        nbtweets = len(tweetsdf.TWEETID.unique())\n",
    "        print(\"Nombre de tweets uniques : \",nbtweets)\n",
    "        nsplit = int(nbtweets/10000)+1\n",
    "        ListOfTweetsDF = np.array_split(tweetsdf, nsplit)\n",
    "        \n",
    "        if nbtweets>1000:\n",
    "            print(\"Le traitement commence\")\n",
    "            self.WaitingCorpusList = ListOfTweetsDF\n",
    "            self.LastDate = self.WaitingCorpusList[-1].AUTHORTWEETUNIXEPOCH.max()\n",
    "        else:\n",
    "            print(\"Pas assez de tweets à traiter, réessayer un autre moment\")\n",
    "            self.WaitingCorpusList = []\n",
    "            \n",
    "            \n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "    # TRAITEMENT DE LA LISTE DE BATCH\n",
    "    def ComputeCorpus(self):\n",
    "        for iddf,df in enumerate(self.WaitingCorpusList):\n",
    "            print(str(iddf+1),\"/\",str(len(self.WaitingCorpusList)))\n",
    "            self.AddCorpus(df)\n",
    "            self.BuildDocsRepresentation()\n",
    "        return None\n",
    "            \n",
    "            \n",
    "    # TRAITEMENT D'UN BATCH\n",
    "    def AddCorpus(self,tweetsdf):     \n",
    "        \n",
    "        if len(tweetsdf)>0:\n",
    "            \n",
    "            # Build les batch df\n",
    "            tweetsdf.TWEETCONTENT = tweetsdf.TWEETCONTENT.map(lambda a : Tokenizer(a))\n",
    "            Words2TweetsDF = DeplyrDF(tweetsdf)\n",
    "            CompteurDic = BooleanCorpusCompteur(tweetsdf.TWEETCONTENT.tolist())\n",
    "            V = list(CompteurDic.keys())\n",
    "            DocsTouched = BuildDocumentsTouched(V,tweetsdf.TWEETCONTENT.tolist())\n",
    "            DocsTouched = pd.DataFrame(data=DocsTouched,index=[0]).T.reset_index().rename(columns = {\"index\":\"word\",0:\"f\"})\n",
    "            \n",
    "            # Remove les mots trop peu fréquents du batch\n",
    "            TooFrequentRaw = int(round(self.TooFrequentThreshold * len(Words2TweetsDF.tweetid.unique())))\n",
    "            TooInfrequentRaw = int(round(self.TooInfrequentThreshold * len(Words2TweetsDF.tweetid.unique())))\n",
    "            self.TooInfrequentRaw = TooInfrequentRaw\n",
    "            self.TooFrequentRaw = TooFrequentRaw\n",
    "            self.Toovalue = len(Words2TweetsDF.tweetid.unique())\n",
    "            DocsTouched = DocsTouched.copy()[(DocsTouched.f>TooInfrequentRaw) & (DocsTouched.f<TooFrequentRaw)]\n",
    "            Words2TweetsDF = Words2TweetsDF.merge(DocsTouched,on=\"word\")\n",
    "            Words2TweetsDF = Words2TweetsDF[[\"word\",\"tweetid\"]]\n",
    "            V = DocsTouched.word.tolist()\n",
    "            \n",
    "            # Assignation des batch df\n",
    "            self.BATCH_WORDS2TWEETS = Words2TweetsDF\n",
    "            self.BATCH_DOCSTOUCHED = DocsTouched\n",
    "            self.BATCH_COMPTEUR = CompteurDic\n",
    "            self.BATCH_V = V\n",
    "\n",
    "            \n",
    "                                                                 \n",
    "                                                                 \n",
    "            #  * * * UPDATING PART * * * \n",
    "            #  * * * UPDATING PART * * *\n",
    "            #  * * * UPDATING PART * * *\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # maj du dic\n",
    "            self.COMPTEUR = UpdateCompteurDic(self.COMPTEUR,self.BATCH_COMPTEUR)\n",
    "            \n",
    "            # maj du words2tweets\n",
    "            self.WORDS2TWEETS = pd.concat((self.WORDS2TWEETS,self.BATCH_WORDS2TWEETS),axis=0)\n",
    "            \n",
    "            # maj du docstouched\n",
    "            if len(self.DOCSTOUCHED)>0:\n",
    "                fil = self.DOCSTOUCHED.word.isin(pd.Series(V))\n",
    "                nepastoucher = self.DOCSTOUCHED[~fil]\n",
    "                amodifier = self.DOCSTOUCHED[fil]\n",
    "                amodifier = pd.concat((amodifier,self.BATCH_DOCSTOUCHED),axis = 0)\n",
    "                amodifier = amodifier.groupby(\"word\")[\"f\"].sum().reset_index()\n",
    "                self.DOCSTOUCHED = pd.concat((amodifier,nepastoucher),axis = 0)\n",
    "            else:\n",
    "                self.DOCSTOUCHED = self.BATCH_DOCSTOUCHED\n",
    "                \n",
    "\n",
    "            \n",
    "        # si tweetsdf est vide alors les batch sont vides\n",
    "        else:\n",
    "            self.BATCH_WORDS2TWEETS = pd.DataFrame()\n",
    "            self.BATCH_DOCSTOUCHED = pd.DataFrame()\n",
    "            self.BATCH_COMPTEUR = {}\n",
    "            self.BATCH_V = []\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def BuildDocsRepresentation(self):\n",
    "        \n",
    "        # Si y'a rien dans le batch alors on s'arrête là\n",
    "        if len(self.BATCH_V)==0:\n",
    "            return None\n",
    "        \n",
    "        # Si y'a déjà des idf scores de calculé on doit la mettre à jour\n",
    "        # Le keepdf, contient les mots non présents dans le batch\n",
    "        if len(self.DocsRepresentation)>0:\n",
    "            fil = self.DocsRepresentation.word.isin(pd.Series(self.BATCH_V))\n",
    "            DocsRepresentationKeep = self.DocsRepresentation.copy()[~fil]\n",
    "            DocsRepresentationKeep[\"D\"] = self.EvaluateD()\n",
    "        else:\n",
    "            DocsRepresentationKeep = pd.DataFrame()\n",
    "        \n",
    "        # df intermediaires\n",
    "        fil = self.DOCSTOUCHED.word.isin(pd.Series(self.BATCH_V))\n",
    "        temptouched = self.DOCSTOUCHED.copy()[fil]\n",
    "        fil = self.WORDS2TWEETS.word.isin(pd.Series(self.BATCH_V))\n",
    "        tempwords2 = self.WORDS2TWEETS[fil]        \n",
    "        toadd = temptouched.merge(tempwords2,on=\"word\")\n",
    "        toadd[\"D\"] = self.EvaluateD()   \n",
    "        \n",
    "        # concaténation des scores des anciens mots et des nouveaux mots\n",
    "        solution = pd.concat((toadd,DocsRepresentationKeep),axis = 0, sort = True)\n",
    "        solution[\"idf\"] = (solution.D / solution.f).map(lambda a : math.log(a))\n",
    "        solution = solution\n",
    "        solution.reset_index(drop = True,inplace = True)\n",
    "        solution.sort_values(by = [\"tweetid\",\"idf\"],ascending=False,inplace=True)\n",
    "        solution.drop_duplicates(inplace = True)\n",
    "        self.DocsRepresentation = solution\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer(randomstring,french_stopwords=french_stopwords):\n",
    "    translator = str.maketrans(string.punctuation,' '*32)\n",
    "    randomstring = randomstring.translate(translator)\n",
    "    randomstring = randomstring.replace(\"’\",\" \")\n",
    "    randomstring = randomstring.replace(\"`\",\" \")\n",
    "    randomstring = randomstring.replace(\"'\",\" \")\n",
    "    randomstring = randomstring.replace(\"“\",\" \")\n",
    "    randomstring = randomstring.replace(\"”\",\" \")\n",
    "    randomstring = randomstring.replace(\"…\",\" \")\n",
    "    randomstring = randomstring.lower()\n",
    "    randomstring = \" \".join(randomstring.split())\n",
    "    words = randomstring.split(\" \")\n",
    "    words = [item for item in words if item not in french_stopwords]\n",
    "    words = [item for item in words if len(item)>2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceCompteur(compteur,words,weight=1):\n",
    "    for w in words:\n",
    "        compteur[w] = compteur.get(w,0) + weight\n",
    "    return compteur\n",
    "\n",
    "def BooleanCorpusCompteur(Corpus):\n",
    "    compteur = {}\n",
    "    for doc in Corpus:\n",
    "        compteur = SentenceCompteur(compteur,doc)\n",
    "    return compteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDocumentsTouched(V,CleanedCorpus):\n",
    "    \n",
    "    DocumentsTouched = {}\n",
    "    for w in V:\n",
    "        for d in CleanedCorpus:\n",
    "            if w in d:\n",
    "                DocumentsTouched[w] = DocumentsTouched.get(w,0) + 1        \n",
    "    \n",
    "    return DocumentsTouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateCompteurDic(OriginalDic,AddDic):\n",
    "    for k,v in AddDic.items():\n",
    "        OriginalDic[k] = OriginalDic.get(k,0) + v\n",
    "    return OriginalDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeplyrDF(TweetsDataFrame):\n",
    "\n",
    "    L = []\n",
    "    for i,row in TweetsDataFrame.iterrows():\n",
    "        tweetid = row[\"TWEETID\"]\n",
    "        tweetcontent = row[\"TWEETCONTENT\"]\n",
    "        tweetcontent = pd.Series(tweetcontent)\n",
    "        tempdf = tweetcontent.to_frame(name = \"word\")\n",
    "        tempdf[\"tweetid\"] = tweetid\n",
    "        L.append(tempdf)\n",
    "\n",
    "    res = pd.concat(L,axis=0)\n",
    "    res.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitTweetsToAnalyse(TweetsToAnalyse,RemoveWordsPeriod):\n",
    "    TweetsToAnalyse[\"TimeElapsed\"] = (TweetsToAnalyse.AUTHORTWEETUNIXEPOCH.max() - TweetsToAnalyse.AUTHORTWEETUNIXEPOCH) / RemoveWordsPeriod\n",
    "    TweetsToAnalyse[\"TimeElapsed\"] = TweetsToAnalyse[\"TimeElapsed\"].astype(int)\n",
    "    idgroup = TweetsToAnalyse.TimeElapsed.unique()\n",
    "    idgroup.sort()\n",
    "    L = []\n",
    "    for idg in idgroup[:-1]:\n",
    "        tempdf = TweetsToAnalyse.copy()[TweetsToAnalyse.TimeElapsed==idg]\n",
    "        L.append(tempdf)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics commence ...\n",
      "Nombre de retweets à traier :  55\n",
      "Nombre de tweets uniques :  55\n",
      "Pas assez de tweets à traiter, réessayer un autre moment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Analytics commence ...\")\n",
    "corpus = Corpus(TooFrequentThreshold,TooInfrequentThreshold)\n",
    "corpus.LoadData()\n",
    "corpus.ProcessCorpus(TweetsToAnalyse)\n",
    "corpus.ComputeCorpus()\n",
    "corpus.SaveOnDisk()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
